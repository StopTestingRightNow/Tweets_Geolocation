{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CharacterCNN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Imports and declarations"
      ],
      "metadata": {
        "id": "vJWxfl7vnu7N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import os\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import geopy.distance"
      ],
      "metadata": {
        "id": "0Zr6BQaYjXYv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "TRAIN_PATH = \"/content/drive/MyDrive/Tweet Geolocation/Data_train\"\n",
        "TEST_PATH = \"/content/drive/MyDrive/Tweet Geolocation/Data_test\"\n",
        "WEIGHTS_PATH = '/content/drive/MyDrive/Tweet Geolocation/Saved_weights'"
      ],
      "metadata": {
        "id": "6On44tZJGpfw"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_directory = os.fsencode(TRAIN_PATH)\n",
        "test_directory = os.fsencode(TEST_PATH)"
      ],
      "metadata": {
        "id": "HW0zGliBNAAf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Getting all countries in the train set. Needed to do this once to figure out the countries dictionary for encoding.\n",
        "#train_countries = set()\n",
        "#    \n",
        "#for file in os.listdir(train_directory):\n",
        "#  filename = os.fsdecode(file)\n",
        "#  data = pd.read_csv(TRAIN_PATH + \"/\" + filename, sep=\";\")\n",
        "#  train_countries.update(data.geo_country.unique())"
      ],
      "metadata": {
        "id": "WpxQFc4JNxdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#test_countries = set()\n",
        "#    \n",
        "#for file in os.listdir(test_directory):\n",
        "#  filename = os.fsdecode(file)\n",
        "#  data = pd.read_csv(TEST_PATH + \"/\" + filename, sep=\";\")\n",
        "#  test_countries.update(data.geo_country.unique())"
      ],
      "metadata": {
        "id": "ojkOal8AXVh0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "countries_num = 18\n",
        "#Coding countries in different languages to the same values\n",
        "def get_key_by_country(countryName):\n",
        "  countries_dict = {'Argentina':1,\n",
        "                    'Argentine':1,\n",
        "                    'Aruba':2,\n",
        "                    'Bolivia':3,\n",
        "                    'Bolivie':3,\n",
        "                    'Brasile':4,\n",
        "                    'Brasilien':4,\n",
        "                    'Brazil':4,\n",
        "                    'Brésil':4,\n",
        "                    'Chile':5,\n",
        "                    'Chili':5,\n",
        "                    'Colombia':6,\n",
        "                    'Curaçao':7,\n",
        "                    'Ecuador':8,\n",
        "                    'Falkland Islands (Malvinas)':9,\n",
        "                    'Fransk Guyana':10,\n",
        "                    'French Guiana':10,\n",
        "                    'Guyana':11,\n",
        "                    'Panama':12,\n",
        "                    'Paraguay':13,\n",
        "                    'Peru':14,\n",
        "                    'Suriname':15,\n",
        "                    'Trinidad and Tobago':16,\n",
        "                    'Uruguay':17,\n",
        "                    'Venezuela':18\n",
        "                    }\n",
        "  try:\n",
        "    return countries_dict[countryName]\n",
        "  except(KeyError):\n",
        "    return 0"
      ],
      "metadata": {
        "id": "HlnxhryoRSRK"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Here should be Unicode encoding, but it is far too computationally-intensive \n",
        "#for my current available hardware, sorry. Had to revert to OHE over a charset.\n",
        "\n",
        "def encode_text(input_text:str):\n",
        "  tweet_len = 280\n",
        "\n",
        "  input_text = str(input_text).lower()\n",
        "  charset = '0123456789 aáãbcdeéfghiíjklmnñoópqrstuúüvwxyz&@#_,.'\n",
        "  char_num = len(charset)\n",
        "  code = torch.zeros(tweet_len, char_num)\n",
        "  for i, c in enumerate(input_text):\n",
        "    try:\n",
        "      char_code = charset.index(c)\n",
        "      code[min(i,279),char_code] = 1 #min - in case one charater gets enumerated into several characters\n",
        "    except(ValueError):\n",
        "      continue\n",
        "\n",
        "  return code.permute(1,0)"
      ],
      "metadata": {
        "id": "26TDeWXrhDjs"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preparation"
      ],
      "metadata": {
        "id": "mxwarH29ngjN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pandas.core.frame import DataFrame\n",
        "class TweetsDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, input_dataframe:DataFrame):\n",
        "        input_dataframe = input_dataframe.sample(frac=1)\n",
        "        self.len = input_dataframe.shape[0]\n",
        "        self.text = input_dataframe.text\n",
        "        self.target_country = input_dataframe.geo_country\n",
        "        self.target_latitude = input_dataframe.latitude\n",
        "        self.target_longitude = input_dataframe.longitude\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.len\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return encode_text(self.text[index]), \\\n",
        "        torch.tensor(get_key_by_country(self.target_country[index])), \\\n",
        "        torch.tensor((float(self.target_latitude[index]), float(self.target_longitude[index])))"
      ],
      "metadata": {
        "id": "Pha_idzGnpr_"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model preparation"
      ],
      "metadata": {
        "id": "evhgscv3tX-9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TweetCharacterCNN(nn.Module):\n",
        "    def __init__(self, charset_len = 51, countries_num = 18, classifier_dim = 200):\n",
        "      super().__init__() #In -> 280\n",
        "      self.conv1 = nn.Conv1d(in_channels = charset_len, out_channels = charset_len, kernel_size=5) #Out -> 276\n",
        "      self.maxp1 = nn.MaxPool1d(3) #Out -> 92\n",
        "      self.conv2 = nn.Conv1d(in_channels = charset_len, out_channels = charset_len, kernel_size=6) #Out -> 87\n",
        "      self.maxp2 = nn.MaxPool1d(3) #Out -> 29\n",
        "      self.conv3 = nn.Conv1d(in_channels = charset_len, out_channels = charset_len, kernel_size=3) #Out -> 27\n",
        "      self.conv4 = nn.Conv1d(in_channels = charset_len, out_channels = charset_len, kernel_size=3) #Out -> 25\n",
        "      self.conv5 = nn.Conv1d(in_channels = charset_len, out_channels = charset_len, kernel_size=3) #Out -> 23\n",
        "      self.conv6 = nn.Conv1d(in_channels = charset_len, out_channels = charset_len, kernel_size=3) #Out -> 21\n",
        "      \n",
        "      self.cc1activation = nn.ReLU()\n",
        "      self.cc1bn = nn.BatchNorm1d(21*charset_len)\n",
        "      self.country_classifier1 = nn.Linear(21*charset_len, classifier_dim)\n",
        "      self.cc2activation = nn.ReLU()\n",
        "      self.cc2bn = nn.BatchNorm1d(classifier_dim)\n",
        "      self.country_classifier2 = nn.Linear(classifier_dim, countries_num+1)\n",
        "      \n",
        "      self.cr1activation = nn.ReLU()\n",
        "      self.cr1bn = nn.BatchNorm1d(21*charset_len)\n",
        "      self.coord_regressor1 = nn.Linear(21*charset_len, classifier_dim)\n",
        "      self.cr2activation = nn.ReLU()\n",
        "      self.cr2bn = nn.BatchNorm1d(classifier_dim)\n",
        "      self.coord_regressor2 = nn.Linear(classifier_dim, 2)\n",
        "        \n",
        "    def convs(self, x):\n",
        "      x = self.conv1(x)\n",
        "      x = self.maxp1(x)\n",
        "      x = self.conv2(x)\n",
        "      x = self.maxp2(x)\n",
        "      x = self.conv3(x)\n",
        "      x = self.conv4(x)\n",
        "      x = self.conv5(x)\n",
        "      x = self.conv6(x)\n",
        "      return x\n",
        "        \n",
        "    def forward(self, x):\n",
        "      x = self.conv1(x)\n",
        "      x = self.maxp1(x)\n",
        "      x = self.conv2(x)\n",
        "      x = self.maxp2(x)\n",
        "      x = self.conv3(x)\n",
        "      x = self.conv4(x)\n",
        "      x = self.conv5(x)\n",
        "      x = self.conv6(x)\n",
        "      x = torch.flatten(x, start_dim=1) #for batches\n",
        "\n",
        "      country = self.cc1activation(x)\n",
        "      country = self.cc1bn(country)\n",
        "      country = self.country_classifier1(country)\n",
        "      country = self.cc2activation(country)\n",
        "      country = self.cc2bn(country)\n",
        "      country = self.country_classifier2(country)\n",
        "      \n",
        "      x = self.cr1activation(x)\n",
        "      x = self.cr1bn(x)\n",
        "      x = self.coord_regressor1(x)\n",
        "      x = self.cr2activation(x)\n",
        "      x = self.cr2bn(x)\n",
        "      x = self.coord_regressor2(x)\n",
        "\n",
        "      return country, x"
      ],
      "metadata": {
        "id": "GX5y-lpDteIM"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "net = TweetCharacterCNN(classifier_dim = 200)\n",
        "\n",
        "#Here should be Mixture of von Mises-Fisher Distributions for coordinates loss, \n",
        "#but I don't currently have enough computational resources for it, so have to \n",
        "#use Euclidean approximation. Just pretend that the Earth is flat for now, please =)\n",
        "\n",
        "coord_criterion = nn.MSELoss()\n",
        "country_criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "LxbG_KGh0eVj"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.Adam(net.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "5bd6qu8Pobk5"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Training"
      ],
      "metadata": {
        "id": "cbyovShE7vPq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net.train()\n",
        "train_min_length = 5000\n",
        "data_collected = pd.DataFrame(columns=[\"text\",\"geo_country\",\"latitude\",\"longitude\"])\n",
        "\n",
        "iteration = 0\n",
        "filelist = os.listdir(train_directory)\n",
        "random.shuffle(filelist)\n",
        "curr_files = []\n",
        "files_total = len(filelist)\n",
        "files_processed = 0\n",
        "\n",
        "for file in filelist:  \n",
        "  filename = os.fsdecode(file)\n",
        "  curr_files.append(filename)\n",
        "  data = pd.read_csv(TRAIN_PATH + \"/\" + filename, sep=\";\")\n",
        "  files_processed += 1\n",
        "  if data.shape[0] == 0: continue #Skip empty files\n",
        "  data_cleaned = data[[\"text\",\"geo_country\"]].copy()\n",
        "  data_cleaned[\"latitude\"] = float(filename[0:filename.index(\"_\")])\n",
        "  data_cleaned[\"longitude\"] = float(filename[filename.index(\"_\")+1:-4])\n",
        "  try: #Records correctness check\n",
        "    try_dataset = TweetsDataset(data_cleaned)\n",
        "    try_dataloader = torch.utils.data.DataLoader(try_dataset, batch_size=1, shuffle=True, num_workers=2, drop_last = True)\n",
        "    try_get_subscr = next(iter(try_dataloader))\n",
        "    data_collected = pd.concat([data_collected, data_cleaned]) #Accumulating data over several files until threshold\n",
        "  except(TypeError):\n",
        "    print(f'Broken records in file {filename}')\n",
        "  if data_collected.shape[0] >= train_min_length:\n",
        "    data_collected.reset_index(drop=True, inplace=True)\n",
        "    print(f'Datapoints collected: {data_collected.shape[0]}; Total files processed: {files_processed}/{files_total}')\n",
        "    train_dataset = TweetsDataset(data_collected)\n",
        "    train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=100, shuffle=True, num_workers=1, drop_last = True)\n",
        "\n",
        "    Acc_50 = 0\n",
        "    Acc_100 = 0\n",
        "    Acc_500 = 0\n",
        "    Acc_1000 = 0\n",
        "    Acc_2000 = 0\n",
        "    Acc_3000 = 0\n",
        "    Acc_5000 = 0\n",
        "    Acc_country = 0\n",
        "    total_mse_loss = 0\n",
        "    total_CE_loss = 0\n",
        "    total_miss = 0\n",
        "    iteration += 1\n",
        "    try: #skip files with broken records\n",
        "      for i, data in enumerate(train_dataloader):\n",
        "          inputs = data[0]\n",
        "          target_country = data[1]\n",
        "          target_coords = data[2]\n",
        "\n",
        "          #Multitask: first, try to predict country\n",
        "          optimizer.zero_grad()\n",
        "          outputs = net(inputs)\n",
        "          country_loss = country_criterion(outputs[0], target_country)\n",
        "          total_CE_loss += country_loss\n",
        "          country_loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          #Second, try to predict coords\n",
        "          optimizer.zero_grad()\n",
        "          outputs = net(inputs)\n",
        "          coords_loss = coord_criterion(outputs[1], target_coords)\n",
        "          total_mse_loss += coords_loss\n",
        "          coords_loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          #print(f'Cross-Entropy loss = {country_loss}')\n",
        "          #print(f'MSE loss = {coords_loss}')\n",
        "\n",
        "          # Calculating range-based accuracy\n",
        "          pred_country = outputs[0]\n",
        "          pred_coords = outputs[1]\n",
        "\n",
        "          #bounding crazy early results\n",
        "\n",
        "          for i, coords in enumerate(pred_coords):\n",
        "            lat = max(min(coords[0],90),-90)\n",
        "            long = max(min(coords[1],180),-180)\n",
        "            miss_distance = geopy.distance.distance((lat, long), target_coords[i]).km\n",
        "            total_miss += miss_distance\n",
        "            if miss_distance <= 50:\n",
        "              Acc_50 +=1\n",
        "            if miss_distance <= 100:\n",
        "              Acc_100 +=1\n",
        "            if miss_distance <= 500:\n",
        "              Acc_500 +=1\n",
        "            if miss_distance <= 1000:\n",
        "              Acc_1000 +=1\n",
        "            if miss_distance <= 2000:\n",
        "              Acc_2000 +=1\n",
        "            if miss_distance <= 3000:\n",
        "              Acc_3000 +=1\n",
        "            if miss_distance <= 5000:\n",
        "              Acc_5000 +=1\n",
        "\n",
        "          for i, country in enumerate(pred_country):\n",
        "            if torch.argmax(country) == target_country[i]:\n",
        "              Acc_country +=1\n",
        "      total_points = data_collected.shape[0]  \n",
        "      print(f'Iteration: {iteration}, Accuracies:')\n",
        "      print(f'Country - {Acc_country/total_points}, @50 - {Acc_50/total_points},' \\\n",
        "      f'@100 - {Acc_100/total_points}, @500 - {Acc_500/total_points},' \\\n",
        "      f'@1000 - {Acc_1000/total_points}, @2000 - {Acc_2000/total_points},' \\\n",
        "      f'@3000 - {Acc_3000/total_points}, @5000 - {Acc_5000/total_points}')\n",
        "      print(f'Avg losses: MSE - {total_mse_loss/total_points}, CE - {total_CE_loss/total_points}, Distance - {total_miss/total_points}')\n",
        "      torch.save(net.state_dict(), WEIGHTS_PATH + '/geolocation_temp.pt')\n",
        "      data_collected = pd.DataFrame(columns=[\"text\",\"geo_country\",\"latitude\",\"longitude\"])      \n",
        "      curr_files = []\n",
        "    except(TypeError):\n",
        "      print(f'Broken records in batch with files {curr_files}')  \n",
        "      data_collected = pd.DataFrame(columns=[\"text\",\"geo_country\",\"latitude\",\"longitude\"])  \n",
        "      curr_files = []\n",
        "        \n",
        "print('Training is finished!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 695
        },
        "id": "EbEy2UQn7lkz",
        "outputId": "46f01e4c-8c58-4290-b87f-86eede5ad53a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datapoints collected: 7155; Total files processed: 3/3278\n",
            "Iteration: 1, Accuracies:\n",
            "Country - 0.8867924528301887, @50 - 0.0004192872117400419,@100 - 0.0023759608665269044, @500 - 0.27882599580712786,@1000 - 0.4863731656184486, @2000 - 0.5306778476589797,@3000 - 0.5703703703703704, @5000 - 0.6489168413696715\n",
            "Avg losses: MSE - 8.692052841186523, CE - 0.006238440051674843, Distance - 3277.849283941884\n",
            "Datapoints collected: 29208; Total files processed: 13/3278\n",
            "Iteration: 2, Accuracies:\n",
            "Country - 0.9439194741166803, @50 - 0.05293070391673514,@100 - 0.19237880032867707, @500 - 0.9134826075047932,@1000 - 0.9535401259928786, @2000 - 0.962236373596275,@3000 - 0.9721993974253629, @5000 - 0.9996576280471103\n",
            "Avg losses: MSE - 0.22839267551898956, CE - 0.0034022070467472076, Distance - 340.7054591154428\n",
            "Datapoints collected: 54164; Total files processed: 18/3278\n",
            "Iteration: 3, Accuracies:\n",
            "Country - 0.988165571228122, @50 - 0.7473044826822244,@100 - 0.9185806070452699, @500 - 0.9791743593530758,@1000 - 0.988885606676021, @2000 - 0.9946274278118308,@3000 - 0.9987999409201683, @5000 - 0.9988184033675505\n",
            "Avg losses: MSE - 0.017636554315686226, CE - 0.0007163697737269104, Distance - 65.09502813760209\n",
            "Datapoints collected: 27946; Total files processed: 40/3278\n",
            "Iteration: 4, Accuracies:\n",
            "Country - 0.9195591497888785, @50 - 0.012345237243254849,@100 - 0.05285192871967365, @500 - 0.6533314248908609,@1000 - 0.8554712660130251, @2000 - 0.9214556644958134,@3000 - 0.9512631503614113, @5000 - 0.996707936735132\n",
            "Avg losses: MSE - 0.517524778842926, CE - 0.0040104142390191555, Distance - 661.8906349896009\n",
            "Datapoints collected: 53944; Total files processed: 42/3278\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-c5219180ee20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     63\u001b[0m           \u001b[0mcoords_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcoord_criterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_coords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m           \u001b[0mtotal_mse_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcoords_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m           \u001b[0mcoords_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m           \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    173\u001b[0m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[1;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m def grad(\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Saving model"
      ],
      "metadata": {
        "id": "PkI27NcfJohq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(net.state_dict(), WEIGHTS_PATH + '/geolocation.pt')"
      ],
      "metadata": {
        "id": "A5QFSI4TJvOy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Loading model"
      ],
      "metadata": {
        "id": "lHbaXD7_J7Px"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "net = TweetCharacterCNN(classifier_dim = 200)\n",
        "net.load_state_dict(torch.load(WEIGHTS_PATH + '/geolocation.pt'))\n",
        "net.eval()\n",
        "\n",
        "coord_criterion = nn.MSELoss()\n",
        "country_criterion = nn.CrossEntropyLoss()"
      ],
      "metadata": {
        "id": "gWFccq1EJ_HB"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Getting prediction"
      ],
      "metadata": {
        "id": "EpCxzlu5vknp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "countries_dec_dict = {1:'Argentina',\n",
        "                    2:'Aruba',\n",
        "                    3:'Bolivia',\n",
        "                    4:'Brazil',\n",
        "                    5:'Chile',\n",
        "                    6:'Colombia',\n",
        "                    7:'Curaçao',\n",
        "                    8:'Ecuador',\n",
        "                    9:'Falkland Islands (Malvinas)',\n",
        "                    10:'French Guiana',\n",
        "                    11:'Guyana',\n",
        "                    12:'Panama',\n",
        "                    13:'Paraguay',\n",
        "                    14:'Peru',\n",
        "                    15:'Suriname',\n",
        "                    16:'Trinidad and Tobago',\n",
        "                    17:'Uruguay',\n",
        "                    18:'Venezuela'\n",
        "                    }"
      ],
      "metadata": {
        "id": "sbEZrKvowE1q"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Running test dataset\n",
        "net.eval()\n",
        "test_min_length = 5000\n",
        "\n",
        "Acc_50 = 0\n",
        "Acc_100 = 0\n",
        "Acc_500 = 0\n",
        "Acc_1000 = 0\n",
        "Acc_2000 = 0\n",
        "Acc_3000 = 0\n",
        "Acc_5000 = 0\n",
        "Acc_country = 0\n",
        "total_mse_loss = 0\n",
        "total_CE_loss = 0\n",
        "total_miss = 0\n",
        "total_points = 0\n",
        "\n",
        "data_collected = pd.DataFrame(columns=[\"text\",\"geo_country\",\"latitude\",\"longitude\"])\n",
        "\n",
        "filelist = os.listdir(test_directory)\n",
        "random.shuffle(filelist)\n",
        "\n",
        "for file in filelist:  \n",
        "  filename = os.fsdecode(file)\n",
        "  data = pd.read_csv(TEST_PATH + \"/\" + filename, sep=\";\")\n",
        "  if data.shape[0] == 0: continue #Skip empty files\n",
        "  data_cleaned = data[[\"text\",\"geo_country\"]].copy()\n",
        "  data_cleaned[\"latitude\"] = float(filename[0:filename.index(\"_\")])\n",
        "  data_cleaned[\"longitude\"] = float(filename[filename.index(\"_\")+1:-4])\n",
        "  data_collected = pd.concat([data_collected, data_cleaned]) #Accumulating data from all test set\n",
        "  if data_collected.shape[0] >= test_min_length:    \n",
        "    total_points += data_collected.shape[0]  \n",
        "    data_collected.reset_index(drop=True, inplace=True)\n",
        "    print(f'Datapoints collected: {data_collected.shape[0]}')\n",
        "    test_dataset = TweetsDataset(data_collected)\n",
        "    test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=100, shuffle=True, num_workers=1, drop_last = True)\n",
        "\n",
        "    for i, data in enumerate(test_dataloader):\n",
        "        inputs = data[0]\n",
        "        target_country = data[1]\n",
        "        target_coords = data[2]\n",
        "\n",
        "        #Multitask: first, try to predict country\n",
        "        outputs = net(inputs)\n",
        "        country_loss = country_criterion(outputs[0], target_country)\n",
        "        total_CE_loss += country_loss\n",
        "\n",
        "        #Second, try to predict coords\n",
        "        outputs = net(inputs)\n",
        "        coords_loss = coord_criterion(outputs[1], target_coords)\n",
        "        total_mse_loss += coords_loss\n",
        "\n",
        "        #print(f'Cross-Entropy loss = {country_loss}')\n",
        "        #print(f'MSE loss = {coords_loss}')\n",
        "\n",
        "        # Calculating range-based accuracy\n",
        "        pred_country = outputs[0]\n",
        "        pred_coords = outputs[1]\n",
        "\n",
        "        #bounding crazy results\n",
        "\n",
        "        for i, coords in enumerate(pred_coords):\n",
        "          lat = max(min(coords[0],90),-90)\n",
        "          long = max(min(coords[1],180),-180)\n",
        "          miss_distance = geopy.distance.distance((lat, long), target_coords[i]).km\n",
        "          total_miss += miss_distance\n",
        "          if miss_distance <= 50:\n",
        "            Acc_50 +=1\n",
        "          if miss_distance <= 100:\n",
        "            Acc_100 +=1\n",
        "          if miss_distance <= 500:\n",
        "            Acc_500 +=1\n",
        "          if miss_distance <= 1000:\n",
        "            Acc_1000 +=1\n",
        "          if miss_distance <= 2000:\n",
        "            Acc_2000 +=1\n",
        "          if miss_distance <= 3000:\n",
        "            Acc_3000 +=1\n",
        "          if miss_distance <= 5000:\n",
        "            Acc_5000 +=1\n",
        "\n",
        "        for i, country in enumerate(pred_country):\n",
        "          if torch.argmax(country) == target_country[i]:\n",
        "            Acc_country +=1\n",
        "print(f'Accuracies:')\n",
        "print(f'Country - {Acc_country/total_points}, @50 - {Acc_50/total_points},' \\\n",
        "f'@100 - {Acc_100/total_points}, @500 - {Acc_500/total_points},' \\\n",
        "f'@1000 - {Acc_1000/total_points}, @2000 - {Acc_2000/total_points},' \\\n",
        "f'@3000 - {Acc_3000/total_points}, @5000 - {Acc_5000/total_points}')\n",
        "print(f'Avg losses: MSE - {total_mse_loss/total_points}, CE - {total_CE_loss/total_points}, Distance - {total_miss/total_points}')\n",
        "print('Testing is finished!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XYGOGOs8zKtb",
        "outputId": "10a540a2-5fc0-4d37-daee-3d283aabd58e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Datapoints collected: 8840\n",
            "Datapoints collected: 9031\n",
            "Datapoints collected: 9222\n"
          ]
        }
      ]
    }
  ]
}